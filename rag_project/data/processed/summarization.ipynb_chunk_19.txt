 to the context window of a LLM For long texts we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a models context window size Here we implement a recursive collapsing of the summaries the inputs are partitioned based on a token limit and summaries are generated of the partitions This step is repeated until the total length of the summaries is within a desired limit allowing for the summarization of arbitrarylength text First we chunk the blog post into sma