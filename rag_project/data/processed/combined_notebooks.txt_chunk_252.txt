ning chatbot However one really important UX consideration for chatbot applications is streaming LLMs can sometimes take a while to respond and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated This allows the user to see progress Its actually super easy to do this By default stream in our LangGraph application streams application steps in this case the single step of the model response Setting stream_modemessages allows us to stream 