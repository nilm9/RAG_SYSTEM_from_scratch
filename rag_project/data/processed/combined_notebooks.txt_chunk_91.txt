Runnablesdocsconceptsrunnables they expose a standard interface that includes async and streaming modes of invocation This allows us to stream individual tokens from a chat model for token in modelstreammessages printtokencontent end You can find more details on streaming chat model outputs in this guidedocshow_tochat_streaming  Prompt Templates Right now we are passing a list of messages directly into the language model Where does this list of messages come from Usually it is constructed from a combination