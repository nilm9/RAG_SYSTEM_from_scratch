prompt and pass that prompt to an LLM  Streaming Note that we can also stream the result tokenbytoken  Go deeper  You can easily customize the prompt  You can easily try different LLMs eg Claudedocsintegrationschatanthropic via the llm parameter  MapReduce summarize long texts via parallelization mapreduce Lets unpack the map reduce approach For this well first map each document to an individual summary using an LLM Then well reduce or consolidate those summaries into a single global summary Note that the m