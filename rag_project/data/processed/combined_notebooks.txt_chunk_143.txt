printresult  Streaming Note that we can also stream the result tokenbytoken for token in chainstreamcontext docs printtoken end  Go deeper  You can easily customize the prompt  You can easily try different LLMs eg Claudedocsintegrationschatanthropic via the llm parameter  MapReduce summarize long texts via parallelization mapreduce Lets unpack the map reduce approach For this well first map each document to an individual summary using an LLM Then well reduce or consolidate those summaries into a single glob